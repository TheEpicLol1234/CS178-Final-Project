{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a7c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4b33be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"diabetic_data.csv\")\n",
    "ids_mapping = pd.read_csv(\"IDS_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0839a0a",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d1e9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict whether or not hospital patient will be readmitted within 30 days\n",
    "\n",
    "# Replace \"?\" with NaN to standardize missing values\n",
    "df = df.replace(\"?\", np.nan)\n",
    "\n",
    "# Target variable includes those recently readmitted (within 30 days)\n",
    "# creates a binary target where 1 = readmitted <30, 0 otherise\n",
    "if \"readmitted\" in df.columns:\n",
    "    df[\"target\"] = df[\"readmitted\"].apply(lambda x: 1 if x == \"<30\" else 0)\n",
    "    df = df.drop([\"readmitted\"], axis=1)\n",
    "    \n",
    "# Drop ID and unhelpful columns\n",
    "drop_cols = [\"encounter_id\", \"patient_nbr\", \"weight\", \"payer_code\", \"medical_specialty\", \"examide\",  \"citoglipton\", \"metformin-rosiglitazone\", \"metformin-pioglitazone\"]\n",
    "df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Drop columns with excessive missing values or high cardinality\n",
    "df = df.loc[:, df.isnull().mean() < 0.5]\n",
    "df = df.drop(columns=[col for col in df.select_dtypes(include=\"object\") if df[col].nunique() > 50])\n",
    "df = df.dropna()\n",
    "\n",
    "# One-hot encode with float dtype to avoid object dtype issues\n",
    "df_dummies = pd.get_dummies(df, drop_first=True, dtype=float)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_dummies.drop(\"target\", axis=1)\n",
    "y = df_dummies[\"target\"]\n",
    "\n",
    "# Convert to scipy sparse matrix\n",
    "X_sparse = sparse.csr_matrix(X.values)\n",
    "\n",
    "# Train/test split (sparse matrix version)\n",
    "X_train_sparse, X_test_sparse, y_train, y_test = train_test_split(\n",
    "    X_sparse, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# Feature scaling using sparse-compatible scaler (no centering)\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_scaled = scaler.fit_transform(X_train_sparse)\n",
    "X_test_scaled = scaler.transform(X_test_sparse)\n",
    "\n",
    "# Prepare to store metrics for later model evaluation\n",
    "metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fdb90-2abb-4e05-9a4a-2f271c473082",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    decided to use t-sne because it allows us to turn this multi dimensional space into a 2d space\n",
    "    therefore, we can see what model is the best to use, as a guess  \n",
    "    then we can compare THIS inital guess to the FINAL result\n",
    "'''\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "X_embedded = tsne.fit_transform(X_train_scaled.toarray())\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "# 0 (NOT readmitted) = blue || 1 (readmitted) = red\n",
    "scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y_train, cmap=\"coolwarm\", alpha=0.6)\n",
    "plt.colorbar(scatter, label=\"Readmitted (<30 days = 1)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dbd5a5",
   "metadata": {},
   "source": [
    "k-Nearest Neighors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66c96814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "k = 1\n",
      "Accuracy: 0.8108\n",
      "Precision: 0.1540\n",
      "Recall: 0.1525\n",
      "F1: 0.1532\n",
      "AUC: 0.5233\n"
     ]
    }
   ],
   "source": [
    "'''# Convert sparse matrices to dense arrays (KNN doesn't support sparse input)\n",
    "X_train_dense = X_train_scaled.toarray()\n",
    "X_test_dense = X_test_scaled.toarray()\n",
    "\n",
    "metrics[\"kNN\"] = {}\n",
    "\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_dense, y_train)\n",
    "    \n",
    "    y_pred_knn = knn.predict(X_test_dense)\n",
    "    \n",
    "    metrics[\"kNN\"][k] = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred_knn),\n",
    "        \"Precision\": precision_score(y_test, y_pred_knn, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, y_pred_knn),\n",
    "        \"F1\": f1_score(y_test, y_pred_knn),\n",
    "        \"AUC\": roc_auc_score(y_test, y_pred_knn)\n",
    "    }\n",
    "\n",
    "    print(f\"\\nk = {k}\")\n",
    "    for metric_name, value in metrics[\"kNN\"][k].items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "# Extract metric values for each k\n",
    "ks = list(metrics[\"kNN\"].keys())\n",
    "accuracy = [metrics[\"kNN\"][k][\"Accuracy\"] for k in ks]\n",
    "precision = [metrics[\"kNN\"][k][\"Precision\"] for k in ks]\n",
    "recall = [metrics[\"kNN\"][k][\"Recall\"] for k in ks]\n",
    "f1 = [metrics[\"kNN\"][k][\"F1\"] for k in ks]\n",
    "auc = [metrics[\"kNN\"][k][\"AUC\"] for k in ks]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(ks, accuracy, label=\"Accuracy\", marker='o')\n",
    "plt.plot(ks, precision, label=\"Precision\", marker='o')\n",
    "plt.plot(ks, recall, label=\"Recall\", marker='o')\n",
    "plt.plot(ks, f1, label=\"F1 Score\", marker='o')\n",
    "plt.plot(ks, auc, label=\"AUC\", marker='o')\n",
    "\n",
    "plt.xlabel(\"Number of Neighbors (k)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"KNN Classifier Metrics vs k\")\n",
    "plt.xticks(ks)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "\n",
    "# Convert sparse matrices to dense arrays (kNN doesn't support sparse input)\n",
    "X_train_dense = X_train_scaled.toarray()\n",
    "X_test_dense = X_test_scaled.toarray()\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 21)),\n",
    "    'weights': ['uniform', 'distance'],  # optional\n",
    "    'metric': ['euclidean']  # you can add 'manhattan', etc.\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search_knn = GridSearchCV(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "grid_search_knn.fit(X_train_dense, y_train)\n",
    "\n",
    "# Best model\n",
    "best_knn = grid_search_knn.best_estimator_\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_knn = best_knn.predict(X_test_dense)\n",
    "\n",
    "# Store metrics\n",
    "metrics[\"kNN\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred_knn),\n",
    "    \"Precision\": precision_score(y_test, y_pred_knn, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, y_pred_knn),\n",
    "    \"F1\": f1_score(y_test, y_pred_knn),\n",
    "    \"AUC\": roc_auc_score(y_test, best_knn.predict_proba(X_test_dense)[:, 1])\n",
    "}\n",
    "\n",
    "# Print metrics\n",
    "print(f\"\\nBest k: {best_knn.n_neighbors}\")\n",
    "for metric_name, value in metrics[\"kNN\"].items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838133f4",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for logistic regression\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs'],  # 'lbfgs' only supports l2\n",
    "    'class_weight': ['balanced'],\n",
    "    'max_iter': [1000],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Initialize base model\n",
    "base_model = LogisticRegression()\n",
    "\n",
    "# Set up GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Retrieve best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_lr = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate and store metrics\n",
    "metrics[\"Logistic Regression\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred_lr),\n",
    "    \"Precision\": precision_score(y_test, y_pred_lr),\n",
    "    \"Recall\": recall_score(y_test, y_pred_lr),\n",
    "    \"F1\": f1_score(y_test, y_pred_lr),\n",
    "    \"AUC\": roc_auc_score(y_test, best_model.predict_proba(X_test_scaled)[:, 1])\n",
    "}\n",
    "\n",
    "# Print metrics\n",
    "for metric_name, value in metrics[\"Logistic Regression\"].items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9350c09c",
   "metadata": {},
   "source": [
    "Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(64,), (64, 32), (128, 64), (128, 64, 32)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # L2 regularization term\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [1000],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_mlp = GridSearchCV(\n",
    "    estimator=MLPClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search_mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best model\n",
    "best_mlp = grid_search_mlp.best_estimator_\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_mlp = best_mlp.predict(X_test_scaled)\n",
    "\n",
    "# Store metrics\n",
    "metrics[\"Feedforward NN\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred_mlp),\n",
    "    \"Precision\": precision_score(y_test, y_pred_mlp),\n",
    "    \"Recall\": recall_score(y_test, y_pred_mlp),\n",
    "    \"F1\": f1_score(y_test, y_pred_mlp),\n",
    "    \"AUC\": roc_auc_score(y_test, best_mlp.predict_proba(X_test_scaled)[:, 1])\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nBest Params (MLP): {grid_search_mlp.best_params_}\")\n",
    "for metric_name, value in metrics[\"Feedforward NN\"].items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95557f4c",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f20ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "max_depth = 59\n",
      "Accuracy: 0.8001\n",
      "Precision: 0.1592\n",
      "Recall: 0.1823\n",
      "F1: 0.1700\n",
      "AUC: 0.5303\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "metrics[\"Decision Tree\"] = {}\n",
    "\n",
    "for depth in range(1,60):\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    dt.fit(X_train_scaled, y_train)\n",
    "    y_pred_dt = dt.predict(X_test_scaled)\n",
    "    \n",
    "    metrics[\"Decision Tree\"][depth] = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred_dt),\n",
    "        \"Precision\": precision_score(y_test, y_pred_dt, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, y_pred_dt),\n",
    "        \"F1\": f1_score(y_test, y_pred_dt),\n",
    "        \"AUC\": roc_auc_score(y_test, y_pred_dt)\n",
    "    }\n",
    "    print(f\"\\nmax_depth = {depth}\")\n",
    "    for metric_name, value in metrics[\"Decision Tree\"][depth].items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "'''\n",
    "dt = DecisionTreeClassifier(max_depth=32, random_state=42)\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "y_pred_dt = dt.predict(X_test_scaled)\n",
    "\n",
    "metrics[\"Decision Tree\"][depth] = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred_dt),\n",
    "    \"Precision\": precision_score(y_test, y_pred_dt, zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, y_pred_dt),\n",
    "    \"F1\": f1_score(y_test, y_pred_dt),\n",
    "    \"AUC\": roc_auc_score(y_test, y_pred_dt)\n",
    "}\n",
    "print(f\"\\nmax_depth = {depth}\")\n",
    "for metric_name, value in metrics[\"Decision Tree\"][depth].items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dfcf00",
   "metadata": {},
   "source": [
    "Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ebe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(title, y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion(\"kNN Confusion Matrix\", y_test, y_pred_knn)\n",
    "plot_confusion(\"Logistic Regression Confusion Matrix\", y_test, y_pred_lr)\n",
    "plot_confusion(\"Feedforward NN Confusion Matrix\", y_test, y_pred_mlp)\n",
    "plot_confusion(\"Decision Tree Confusion Matrix\", y_test, y_pred_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46055d1a",
   "metadata": {},
   "source": [
    "Visualization of Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de1e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_dict):\n",
    "    metrics_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUC\"]\n",
    "    classifiers = list(metrics_dict.keys())\n",
    "    values = {metric: [metrics_dict[classifier][metric] for classifier in classifiers] for metric in metrics_names}\n",
    "\n",
    "    x = np.arange(len(classifiers))\n",
    "    width = 0.2\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for i, metric in enumerate(metrics_names):\n",
    "        plt.bar(x + i*width, values[metric], width=width, label=metric)\n",
    "\n",
    "    plt.xticks(x + width*1.5, classifiers)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Metrics by Classifier\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs178_env)",
   "language": "python",
   "name": "cs178_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
